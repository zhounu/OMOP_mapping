{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python code for **A Novel Sentence Transformer-based Natural Language Processing Approach for Schema Mapping of Electronic Health Records to the OMOP Common Data Model**\n",
    "In this tutorial, we describe how to download data from Athena, preprocess data, and train a sentence-transformer for schema mapping to OMOP Common Data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1/ Download Data from Athena â€“ OHDSI Vocabularies Repository\n",
    "\n",
    " To begin, navigate to the [Athena website](https://athena.ohdsi.org/). Read and accept the License Agreement, sign in or create an account, then hit the **download** bottom on the upper right side of the page. You will see a list of datasets like this\n",
    "\n",
    "<img src=\"./asset/download_page.png\" alt=\"download page\" style=\"width:70%;\"/>\n",
    "\n",
    "\n",
    "Select the dataset you need. You may click select all and download all of them.\n",
    "\n",
    "Once selected, it would generate a download link, which will be emailed to you. Download the vocabulary files from the link provided. These files will be in a compressed format, which you should extract to a directory for later use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2/ Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires PyTorch. Please refer to [PyTorch offical website](https://pytorch.org/get-started/locally/) for the tutorial on how to set up PyTorch locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we focus on training a sentence transformer, which is the most essential part of this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your data files\n",
    "data_path = 'path/to/your/data/'  # Modify this to your data directory\n",
    "\n",
    "# Load the concept relationship dataframe\n",
    "df1 = pd.read_csv(f'{data_path}CONCEPT_RELATIONSHIP.csv', sep='\\t')\n",
    "\n",
    "# Collect mapping relationships\n",
    "# Concept 1 is standard concept in OMOP CDM, whereas concept 2 is not.\n",
    "df1 = df1[df1['relationship_id'] == 'Mapped from']\n",
    "df1 = df1[['concept_id_1', 'concept_id_2']]\n",
    "\n",
    "# Load the concepts name table\n",
    "df2 = pd.read_csv(f'{data_path}CONCEPT.csv', sep='\\t')\n",
    "\n",
    "# merge the two dataframes\n",
    "id_to_concept_name = df2.set_index('concept_id')['concept_name'].to_dict()\n",
    "id_to_domain_id = df2.set_index('concept_id')['domain_id'].to_dict()\n",
    "id_to_vocabulary_id = df2.set_index('concept_id')['vocabulary_id'].to_dict()\n",
    "id_to_concept_class_id = df2.set_index('concept_id')['concept_class_id'].to_dict()\n",
    "id_to_standard_concept = df2.set_index('concept_id')['standard_concept'].to_dict()\n",
    "\n",
    "df1['concept_name_1'] = df1['concept_id_1'].map(id_to_concept_name)\n",
    "df1['domain_id_1'] = df1['concept_id_1'].map(id_to_domain_id)\n",
    "df1['vocabulary_id_1'] = df1['concept_id_1'].map(id_to_vocabulary_id)\n",
    "df1['concept_class_id_1'] = df1['concept_id_1'].map(id_to_concept_class_id)\n",
    "df1['standard_concept_1'] = df1['concept_id_1'].map(id_to_standard_concept)\n",
    "\n",
    "df1['concept_name_2'] = df1['concept_id_2'].map(id_to_concept_name)\n",
    "df1['domain_id_2'] = df1['concept_id_2'].map(id_to_domain_id)\n",
    "df1['vocabulary_id_2'] = df1['concept_id_2'].map(id_to_vocabulary_id)\n",
    "df1['concept_class_id_2'] = df1['concept_id_2'].map(id_to_concept_class_id)\n",
    "df1['standard_concept_2'] = df1['concept_id_2'].map(id_to_standard_concept)\n",
    "\n",
    "# Using the df1 above, you can filter the dataset according to its domain_id, vocabulary_id, etc, then you can collect a subset as your training data\n",
    "\n",
    "# Removing Na\n",
    "df3 = df1.dropna(subset=['domain_id_1'])\n",
    "print(df3.shape)\n",
    "\n",
    "# Confirm that concept 1 is standard, whereas concept 2 is not.\n",
    "df3 = df3[df3['standard_concept_2'] != 'S']\n",
    "df3 = df3[df3['standard_concept_1'] == 'S']\n",
    "print(df3.shape)\n",
    "\n",
    "# Remove records when concept 1 and 2 are exactly the same\n",
    "df3 = df3[df3['concept_class_id_1'] != df3['concept_class_id_2']]\n",
    "df4 = df3[['concept_name_1', 'concept_name_2', 'concept_id_1', 'concept_id_2']]\n",
    "df4 = df4[df4['concept_name_1']!=df4['concept_name_2']]\n",
    "\n",
    "# Now we get all non-standard to standard mapping on Athena\n",
    "df4.to_csv(f'{data_path}omop-non-standard-to-standard-mapping-pairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can collect synonyms and their corresponding standard concepts in OMOP CDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------concept synonyms--------------\n",
    "# Load datasets\n",
    "df = pd.read_csv(f'{data_path}CONCEPT_SYNONYM.csv', sep='\\t', on_bad_lines = 'skip', low_memory = False)\n",
    "df2 = pd.read_csv(f'{data_path}CONCEPT.csv', sep = '\\t')\n",
    "\n",
    "# Collect synonym relationships\n",
    "id_to_concept_name = df2.set_index('concept_id')['concept_name'].to_dict()\n",
    "id_to_domain_id = df2.set_index('concept_id')['domain_id'].to_dict()\n",
    "id_to_standarded_or_not = df2.set_index('concept_id')['standard_concept'].to_dict()\n",
    "df['concept_name'] = df['concept_id'].map(id_to_concept_name)\n",
    "df['domain_id'] = df['concept_id'].map(id_to_domain_id)\n",
    "df['standard_concept'] = df['concept_id'].map(id_to_standarded_or_not)\n",
    "\n",
    "# Drop na\n",
    "df = df.dropna(subset=['domain_id'])\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a more targeted subset, we can uncomment and run the following 3 line in the next code block.   \n",
    "For example, to focus our model on conditions, procedures, and drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['domain_id'].str.contains('Condition|Procedure|Drug')] \n",
    "# print(df.shape)\n",
    "# print(df.domain_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip the code block above, which may result in a larger training set, making the training longer.  \n",
    "Including data from domains that we are not interested in didn't seeem to be very helpful to the model performance, but may yield a model that maps more domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "df = pd.DataFrame({'concept_name_1': df['concept_name'], 'concept_name_2': df['concept_synonym_name'], 'concept_id_1': df['concept_id']})\n",
    "df.to_csv(f'{data_path}omop-synonyms.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3/ Train Sentence-Transformer\n",
    "Now we finally move on to the training of a sentence-transformer model that is more capable in OMOP schema mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we train an existing sentence transformers model\n",
    "from sentence_transformers import SentenceTransformer, models, SentencesDataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "# To only use gpu 0, uncommment the following line\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Load the model\n",
    "# On https://sbert.net/docs/sentence_transformer/pretrained_models.html you can find other sentence transformer models as well\n",
    "# for example, if you have limited GPU resource, you might want to train a smaller model called all-MiniLM-L6-v2\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# You can also build a new sentence-transformer based on an encoder-only model (uncommment the following 3 line)\n",
    "# word_embedding_model = models.Transformer('UFNLP/gatortron-base', max_seq_length=32) # use larger max_seq_length, e.g. 64, if you have more powerful GPU(s)\n",
    "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "# If you want to train the model on more than one GPU, uncommment the following 3 lines\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count() , \"GPUs!\")\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "# Move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the model using synonyms and mapping relationships on Athena\n",
    "synonyms = pd.read_csv(f'{data_path}omop-synonyms.csv')\n",
    "nsts = pd.read_csv(f'{data_path}omop-non-standard-to-standard-mapping-pairs.csv')\n",
    "training_data = pd.concat([synonyms, nsts])\n",
    "print(training_data.head())\n",
    "train_examples = []\n",
    "\n",
    "for _, row in training_data.iterrows():\n",
    "    concept_name_1 = str(row['concept_name_1'])\n",
    "    concept_name_2 = str(row['concept_name_2'])\n",
    "    \n",
    "    # Skip the row if either concept_name_1 or concept_name_2 is not a string\n",
    "    if not isinstance(concept_name_1, str) or not isinstance(concept_name_2, str):\n",
    "        continue\n",
    "    \n",
    "    # Skip the row if either concept_name_1 or concept_name_2 is empty\n",
    "    if not concept_name_1 or not concept_name_2:\n",
    "        continue\n",
    "    \n",
    "    texts = [concept_name_1, concept_name_2]\n",
    "    example = InputExample(texts=texts)\n",
    "    train_examples.append(example)\n",
    "    \n",
    "train_dataset = SentencesDataset(train_examples, model)\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=128)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "num_epochs = 10\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you only have one gpu\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "           epochs=num_epochs,\n",
    "           warmup_steps=warmup_steps,\n",
    "           show_progress_bar=True)\n",
    "\n",
    "# Edit the path to save your model\n",
    "torch.save(model.state_dict(), \"/path/to/the/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have multiple gpus, do not run the code block above; instead, uncomment and run the following lines of Python code\n",
    "# model.module.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "#            epochs=num_epochs,\n",
    "#            warmup_steps=warmup_steps,\n",
    "#            show_progress_bar=True)\n",
    "\n",
    "# torch.save(model.module.state_dict(), \"/path/to/the/model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
